<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Getting started &mdash; WaterfowlDetector 1.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> WaterfowlDetector
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">What is WaterfowlDetector?</a></li>
<li class="toctree-l1"><a class="reference internal" href="Highlights.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Datasets and Pretrained Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="for_developers.html">For Developers</a></li>
<li class="toctree-l1"><a class="reference internal" href="authors.html">Authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">WaterfowlDetector</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Getting started</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/getting_started.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="getting-started">
<h1>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h1>
<p>WaterfowlDetector platform mainly consists of two parts, training and inference.</p>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<section id="data-preparation">
<h3>Data preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h3>
<p>Although we provide pretrained model of our six datasets, we also support customized training on your own bird images. If you want to train you custom datasets, you should prepare your dataset in <a class="reference external" href="https://cocodataset.org/#format-dataformat">COCO</a>  format.</p>
<p>Or, if your high resolution images are labeled by <a class="reference external" href="https://github.com/wkentaro/labelme">labelme</a>, you should have json files which stroe annotation information for each image. In that case provide script can make your dataset prepared for training.
You can run the following line to prepare your training dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">images</span>
<span class="n">python</span> <span class="n">data_preparation</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">p</span> <span class="p">[</span><span class="n">path</span> <span class="n">to</span> <span class="n">your</span> <span class="n">image</span> <span class="n">folder</span><span class="p">]</span>
</pre></div>
</div>
<p>After that, you should have all of your high resolution images cropped and saved in a folder called <code class="file docutils literal notranslate"><span class="pre">all_small</span></code>. In that folder you will find all of cropped images and annotation json files. There is a json file <code class="file docutils literal notranslate"><span class="pre">bird_real.json</span></code> which include all annotation information for images containing birds. cropped images without birds will not include in <code class="file docutils literal notranslate"><span class="pre">bird_real.json</span></code>.</p>
</section>
<section id="training-start">
<h3>Training start<a class="headerlink" href="#training-start" title="Permalink to this headline">¶</a></h3>
<p>After you finish data preparation and configuration, you can start training with one command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train_bird_faster_fpn</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">p</span> <span class="p">[</span><span class="n">path</span> <span class="n">to</span> <span class="n">your</span> <span class="n">image</span> <span class="n">folder</span><span class="p">]</span> <span class="o">-</span><span class="n">m</span> <span class="p">[</span><span class="n">model</span> <span class="n">name</span> <span class="n">you</span> <span class="n">decide</span><span class="p">]</span>
</pre></div>
</div>
<p>The trained model will be saved in <code class="file docutils literal notranslate"><span class="pre">/models/model_name</span></code>. Training weight will be saved in <code class="file docutils literal notranslate"><span class="pre">model_final.pth</span></code></p>
</section>
</section>
<section id="inference-and-evaluation">
<h2>Inference and evaluation<a class="headerlink" href="#inference-and-evaluation" title="Permalink to this headline">¶</a></h2>
<p>Our scripts support inference image with any size. The model infers all low-resolution images and projects all inference results back to high-resolution images.</p>
<section id="inference-with-ground-truth-label">
<h3>Inference with ground truth label<a class="headerlink" href="#inference-with-ground-truth-label" title="Permalink to this headline">¶</a></h3>
<p>If you have ground truth annoation file <code class="file docutils literal notranslate"><span class="pre">bird.json</span></code> in your <code class="file docutils literal notranslate"><span class="pre">all_small</span></code> folder, you can do the inference with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">image_inference_bird</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">p</span> <span class="p">[</span><span class="n">path</span> <span class="n">to</span> <span class="n">your</span> <span class="n">image</span> <span class="n">folder</span><span class="p">]</span> <span class="o">-</span><span class="n">m</span> <span class="p">[</span><span class="n">model</span> <span class="n">name</span><span class="p">]</span> <span class="o">-</span><span class="n">t</span> <span class="p">[</span><span class="n">confidence</span> <span class="n">threshold</span><span class="p">]</span>
</pre></div>
</div>
<p>Instead of MAP and MAR, we use precision, recall and f1-score to evaluate our models performance at the threshold you set. Both inferred images and evaluation results will be saved in your image folder.</p>
</section>
<section id="inference-without-ground-truth-label">
<h3>Inference without ground truth label<a class="headerlink" href="#inference-without-ground-truth-label" title="Permalink to this headline">¶</a></h3>
<p>You can also inference your images although no ground truth labels provided. You can use same command but no evaluation will be generated.</p>
<img alt="_images/Cloud_Ice_60m_test.JPG" class="align-center" src="_images/Cloud_Ice_60m_test.JPG" />
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Yang Zhang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>